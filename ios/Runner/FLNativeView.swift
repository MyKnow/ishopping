import Flutter
import UIKit
import ARKit
import Photos
import Metal
import AVFoundation
import Foundation
import Alamofire
import Vision
import CoreML
import NotificationCenter
import CoreImage


// Fluterì—ì„œ ì‚¬ìš©ì ì •ì˜ í”Œë«í¼ ë·°ë¥¼ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•¨
@available(iOS 17.0, *)
class FLNativeViewFactory: NSObject, FlutterPlatformViewFactory {
    // Flutterì™€ Native ì½”ë“œ ê°„ì˜ í†µì‹ ì„ ìœ„í•œ Flutter ë°”ì´ë„ˆë¦¬ ë©”ì‹ ì €ì— ëŒ€í•œ ì°¸ì¡°ë¥¼ ì €ì¥í•¨
    private var messenger: FlutterBinaryMessenger

    // FlutterBinaryMessengerë¡œ íŒ©í† ë¦¬?ë¥¼ ì´ˆê¸°í™”í•¨
    init(messenger: FlutterBinaryMessenger) {
        self.messenger = messenger
        super.init()
    }

    // FLNativeViewë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•¨
    // ë·°ì˜ í”„ë ˆì„, ì‹ë³„ì, ì„ íƒì  ì¸ì
    func create(
        withFrame frame: CGRect, 
        viewIdentifier viewId: Int64,
        arguments args: Any?
    ) -> FlutterPlatformView {
        return FLNativeView(
            frame: frame,
            viewIdentifier: viewId,
            arguments: args,
            binaryMessenger: messenger)
    }

    // Flutterì™€ ë„¤ì´í‹°ë¸Œ ì½”ë“œ ê°„ì˜ ë©”ì‹œì§€ë¥¼ ì¸ì½”ë”© ë° ë””ì½”ë”©í•˜ê¸° ìœ„í•œ ë©”ì‹œì§€ ì½”ë±ì„ ë°˜í™˜í•¨
    // create ë©”ì†Œë“œì—ì„œ ë¹„-nil ì¸ìë¥¼ ì‚¬ìš©í•  ê²½ìš°ì—ë§Œ í•„ìš”
    public func createArgsCodec() -> FlutterMessageCodec & NSObjectProtocol {
        return FlutterStandardMessageCodec.sharedInstance()
    }
}

// FlutterPlatformView í”„ë¡œí† ì½œì„ êµ¬í˜„í•˜ì—¬ Flutter ë·°ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ
@available(iOS 17.0, *)
class FLNativeView: NSObject, FlutterPlatformView, ARSCNViewDelegate {
    // AR ë‹´ë‹¹ Native View
    private var arView: ARSCNView

    // DepthMap View
    private var depthOverlayView: UIImageView?

    // AR ì„¸ì…˜ êµ¬ì„± ë° ì‹œì‘
    private let configuration = ARWorldTrackingConfiguration()

    private var session: ARSession {
        return arView.session
    }

    // ê°€ì´ë“œ dot ë° ê±°ë¦¬ labelë“¤
    private var gridDots: [UIView] = []
    private var gridLabels: [UILabel] = []

    // ì¡°ì¤€ì  ë° ë¼ë²¨ì˜ ê°¯ìˆ˜
    private final var col: Int = 9
    private final var rw: Int = 21

    // ê¸¸ê²Œ ëˆ„ë¥´ê¸° ì¸ì‹ ì‹œê°„
    private final var longPressTime: Double = 0.5

    // ì˜¤ë²„ë ˆì´ì–´ íˆ¬ëª… ì •ë„
    private final var layerAlpha: CGFloat = 0.9

    // ê±°ë¦¬ì— ë”°ë¥¸ ìƒ‰ìƒì„ ë§¤í•‘í•˜ëŠ” ì‚¬ì „
    private var distanceColorMap: [Float: UIColor] = [
        0.1: .white,
        0.3: .red,
        0.6: .yellow,
        0.9: .orange,
        1.2: .green,
        3.0: .blue,
        6.0: .purple,
        9.0: .black
    ]

    // ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ë“¤ì„ ë°°ì—´ë¡œ ë³€í™˜
    private var indexDistance: [Float] = []

    // Depth map ì˜¤ë²„ë ˆì´ ìƒíƒœë¥¼ ì¶”ì í•˜ëŠ” ë³€ìˆ˜
    private var isDepthMapOverlayEnabled = false

    // Vision ìš”ì²­ì„ ì €ì¥í•  ë°°ì—´
    var requests = [VNRequest]() 

    // ì‚¬ëŒìš© ë°”ìš´ë”© ë°•ìŠ¤ ì €ì¥í•˜ëŠ” ë°°ì—´
    private var humanBoundingBoxViews: [UIView] = []

    // ì‚¬ëŒìš© ë°”ìš´ë”© ë°•ìŠ¤ì™€ì˜ ê±°ë¦¬ë¥¼ ì €ì¥í•˜ëŠ” ë°°ì—´
    private var distanceMeasurements: [Float] = []
    
    // HapticFeedbackManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    let hapticC = HapticFeedbackManager()

    // ImageProcessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    let imageP = ImageProcessor()

    // ARSessionManager ì„ ì–¸
    let arSessionM: ARSessionManager
    
    // ViewController ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€ (í•„ìš”ì— ë”°ë¼)
    var viewController: ViewController?

    private var model: VNCoreMLModel!


    // 
    private var isVibrating: Bool = false

    // 
    private var alertTimer: Timer?


    // ë·°ì˜ í”„ë ˆì„, ë·° ì‹ë³„ì, ì„ íƒì  ì¸ì, ê·¸ë¦¬ê³  ë°”ì´ë„ˆë¦¬ ë©”ì‹ ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤ì´í‹°ë¸Œ ë·°ë¥¼ ì´ˆê¸°í™”
    init( frame: CGRect, viewIdentifier viewId: Int64, arguments args: Any?, binaryMessenger messenger: FlutterBinaryMessenger?) {
        // ARSCNView ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì´ˆê¸°í™”
        arView = ARSCNView(frame: frame)

        arSessionM = ARSessionManager()
        super.init()

        // ì—¬ê¸°ì— ì¡°ê±´ë¬¸ì„ ì¶”ê°€
        if type(of: configuration).supportsFrameSemantics(.sceneDepth) {
            // Activate sceneDepth
            configuration.frameSemantics = .sceneDepth
        }

        //loadModel()

        arView.session = arSessionM.session
        arView.delegate = self
        
        // ViewController ì´ˆê¸°í™”
        viewController = ViewController()
        viewController?.session = arView.session
        
        //NotificationCenter.default.addObserver(self, selector: #selector(appWillResignActive), name: UIApplication.willResignActiveNotification, object: nil)
        //NotificationCenter.default.addObserver(self, selector: #selector(appDidBecomeActive), name: UIApplication.didBecomeActiveNotification, object: nil)

        indexDistance = Array(distanceColorMap.keys).sorted()

        //setupARView()
        arSessionM.runSession()
        setupVision()
        //setupGridDots()
        addShortPressGesture()
        addLongPressGesture()
    }
    deinit {
        arSessionM.pauseSession()
    }

    // ì§§ê²Œ ëˆ„ë¥´ê¸° ì œìŠ¤ì³ ì¶”ê°€
    private func addShortPressGesture() {
        let tapGesture = UITapGestureRecognizer(target: self, action: #selector(handleShortPress))
        arView.addGestureRecognizer(tapGesture)
    }
    // ì§§ê²Œ ëˆ„ë¥´ê¸° ì œìŠ¤ì³ í•¸ë“¤ëŸ¬
    @objc func handleShortPress(_ sender: UITapGestureRecognizer) {
        print("Short Press")
        hapticC.impactFeedback(style: "heavy")
        if let currentFrame = session.currentFrame {
            processFrame(currentFrame)
        }
    }

    // ê¸¸ê²Œ ëˆ„ë¥´ê¸° ì œìŠ¤ì³ ì¶”ê°€
    private func addLongPressGesture() {
        let longPressGesture = UILongPressGestureRecognizer(target: self, action: #selector(handleLongPress))
        longPressGesture.minimumPressDuration = longPressTime // 1ì´ˆ ì´ìƒ ê¸¸ê²Œ ëˆ„ë¥´ê¸°
        arView.addGestureRecognizer(longPressGesture)
    }
    // ê¸¸ê²Œ ëˆ„ë¥´ê¸° ì œìŠ¤ì²˜ í•¸ë“¤ëŸ¬
    @objc func handleLongPress(_ sender: UILongPressGestureRecognizer) {
        if sender.state == .began {
            print(isDepthMapOverlayEnabled)
            hapticC.notificationFeedback(style: "success")
            // Depth map ì˜¤ë²„ë ˆì´ ìƒíƒœ í† ê¸€
            isDepthMapOverlayEnabled.toggle()

            // AR ì„¸ì…˜ì˜ frame semantics ì—…ë°ì´íŠ¸
            if isDepthMapOverlayEnabled {
                configuration.frameSemantics.insert(.sceneDepth)
            } else {
                configuration.frameSemantics.remove(.sceneDepth)
                depthOverlayView?.removeFromSuperview()
                depthOverlayView = nil
            }

            // AR ì„¸ì…˜ ë‹¤ì‹œ ì‹œì‘
            arView.session.run(configuration)
        }
    }

    private func setupVision() {
        // ì‚¬ëŒ ê°ì§€ë¥¼ ìœ„í•œ Vision ìš”ì²­ ì„¤ì •
        let request = VNDetectHumanRectanglesRequest(completionHandler: detectHumanHandler)
        self.requests = [request]
    }

    private func detectHumanHandler(request: VNRequest, error: Error?) {
        DispatchQueue.main.async {
            // ê¸°ì¡´ ê²½ê³„ ìƒì ì œê±°
            self.humanBoundingBoxViews.forEach { $0.removeFromSuperview() }
            self.humanBoundingBoxViews.removeAll()

            guard let observations = request.results as? [VNHumanObservation] else {
                print("No results")
                return
            }

            for observation in observations {
                let boundingBoxView = self.processBoundingBox(for: observation.boundingBox)
                self.arView.addSubview(boundingBoxView)
                self.humanBoundingBoxViews.append(boundingBoxView)
            }
        }
    }

    func processBoundingBox(for boundingBox: CGRect) -> UIView  {
        // í™”ë©´ í¬ê¸°
        let screenSize = self.arView.bounds.size

        // ë””ë°”ì´ìŠ¤ ë°©í–¥
        let orientation = UIDevice.current.orientation

        // ë””ë°”ì´ìŠ¤ ë°©í–¥ì— ë”°ë¼ ì¢Œí‘œë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.
        var x: CGFloat = 0
        var y: CGFloat = 0

        switch orientation {
            case .portrait:
                // `portrait` ëª¨ë“œì—ì„œëŠ” xì™€ y ì¢Œí‘œë¥¼ ì„œë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.
                x = screenSize.width * boundingBox.maxY - (boundingBox.width * screenSize.width)
                y = boundingBox.minX * screenSize.height
            case .landscapeLeft:
                x = boundingBox.minX * screenSize.width
                y = (1 - boundingBox.maxY) * screenSize.height
            //case .landscapeRight:
                // landscape ëª¨ë“œì¼ ë•Œì˜ ì¢Œí‘œ ë³€í™˜
                // ...
            default:
                print("default")
                // ê¸°ë³¸ê°’ ë˜ëŠ” ë‹¤ë¥¸ ë°©í–¥ì¼ ë•Œì˜ ì²˜ë¦¬
                // ...  
        }

        // í™”ë©´ì„ ë²—ì–´ë‚˜ë”ë¼ë„ bounding boxë¥¼ ì˜ë¼ë‚´ì–´ ê³„ì† í‘œì‹œí•˜ë„ë¡ ìˆ˜ì •
        let width = min(boundingBox.width * screenSize.width, screenSize.width - x)
        let height = min(boundingBox.height * screenSize.height, screenSize.height - y)

        // UIKitì˜ ì¢Œí‘œê³„ì— ë§ëŠ” ìœ„ì¹˜ë¡œ UIViewë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        let boundingBoxView = UIView(frame: CGRect(x: x, y: y, width: width, height: height))
        boundingBoxView.layer.borderColor = UIColor.green.cgColor
        boundingBoxView.layer.borderWidth = 2
        boundingBoxView.backgroundColor = .clear

        return boundingBoxView
    }

    func performHitTestAndMeasureDistance() {
        guard let currentFrame = arView.session.currentFrame else {
            //print("Current ARFrame is unavailable.")
            return
        }

        distanceMeasurements.removeAll()

        for boundingBoxView in humanBoundingBoxViews {
            let boxCenter = CGPoint(
                x: boundingBoxView.frame.midX,
                y: boundingBoxView.frame.midY
            )

            if let distance = performHitTesting(boxCenter) {
                distanceMeasurements.append(distance) // ê±°ë¦¬ ì¸¡ì •ê°’ ì €ì¥
            }
        }

        // ê°€ì¥ ì§§ì€ ê±°ë¦¬ ì¶œë ¥
        if let shortestDistance = distanceMeasurements.min() {
            //print("Shortest detected human distance: \(shortestDistance) meters")
            // í–…í‹± í”¼ë“œë°± ë°œìƒ ì¡°ê±´ ì¶”ê°€ (ì˜ˆ: ê±°ë¦¬ê°€ 1ë¯¸í„° ë¯¸ë§Œì¼ ë•Œë§Œ)
            if shortestDistance < 5.0 && !isVibrating {
                isVibrating = true
                if let distance = self.indexDistance.first(where: { $0 > shortestDistance }) {
                    let timeInterval: TimeInterval = TimeInterval(distance)
                    triggerHapticFeedback(interval: timeInterval)
                }
            }
        }
    }

    func triggerHapticFeedback(interval: TimeInterval) {
        hapticC.notificationFeedback(style: "warning")
        let systemSoundID: SystemSoundID
        var delay: TimeInterval = interval
        if delay < 0.7 {
            delay = 0.4
            systemSoundID = 1111
        } else {
            systemSoundID = 1111
        }
        AudioServicesPlaySystemSound(systemSoundID)

        // í–…í‹± í”¼ë“œë°± ì¬ë°œ ë°©ì§€ë¥¼ ìœ„í•´ ì¼ì • ì‹œê°„ ëŒ€ê¸° í›„ isVibrating ì¬ì„¤ì •
        DispatchQueue.global(qos: .userInteractive).asyncAfter(deadline: .now() + (delay*0.5)) {
            DispatchQueue.main.async {
                self.isVibrating = false
            }
        }
    }

    func performHitTesting(_ screenPoint: CGPoint) -> Float? {
        if let hitTestResult = arView.hitTest(screenPoint, types: .featurePoint).first {
            if let currentFrame = arView.session.currentFrame {
                let cameraPosition = currentFrame.camera.transform
                let distance = calculateDistance(from: cameraPosition, to: hitTestResult.worldTransform)
                return distance
            }
        }
        return nil
    }

    private func processFrame(_ frame: ARFrame) {
        let pixelBuffer = frame.capturedImage
        // pixelBufferì—ì„œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ìƒì„± ë° ì²˜ë¦¬

        if let image = imageP.CVPB2UIImage(pixelBuffer: pixelBuffer) {
            imageP.UIImage2PhotoLibrary(image)
            imageP.UIImage2Server(image)
        }
    }

    // ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜
    func calculateDistance(from cameraTransform: simd_float4x4, to hitTransform: matrix_float4x4) -> Float {
        let cameraPosition = cameraTransform.columns.3
        let hitPosition = hitTransform.columns.3
        return sqrt(
            pow(cameraPosition.x - hitPosition.x, 2) +
            pow(cameraPosition.y - hitPosition.y, 2) +
            pow(cameraPosition.z - hitPosition.z, 2)
        )
    }

    private func setupGridDots() {
        let dotSize: CGFloat = 10
        let labelHeight: CGFloat = 20
        let screenWidth = arView.bounds.width
        let screenHeight = arView.bounds.height

        for row in 0..<rw {
            for column in 0..<col {
                // ì  ìƒì„±
                let dot = UIView(frame: CGRect(x: 0, y: 0, width: dotSize, height: dotSize))
                dot.backgroundColor = .red
                dot.layer.cornerRadius = dotSize / 2
                let x = CGFloat(column) * screenWidth / CGFloat(col) + screenWidth / CGFloat(2*col)
                let y = CGFloat(row) * screenHeight / CGFloat(rw) + screenHeight / CGFloat(2*rw)
                dot.center = CGPoint(x: x, y: y)
                arView.addSubview(dot)
                gridDots.append(dot)

                // ë ˆì´ë¸” ìƒì„±
                let label = UILabel(frame: CGRect(x: x - 50, y: y + dotSize, width: 100, height: labelHeight))
                label.textAlignment = .center
                label.textColor = .white
                label.backgroundColor = .black.withAlphaComponent(0.5)
                //arView.addSubview(label)
                gridLabels.append(label)
            }
        }
    }

    // ARSCNViewDelegate ë©”ì„œë“œ
    func renderer(_ renderer: SCNSceneRenderer, updateAtTime time: TimeInterval) {
        DispatchQueue.main.async {
            self.updateGridDotsPosition()
            self.updateDistanceDisplay()

            // ì‚¬ëŒ ê±°ë¦¬ ì¸¡ì •
            self.performHitTestAndMeasureDistance()

            // Depth map ì˜¤ë²„ë ˆì´ê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
            if self.isDepthMapOverlayEnabled, let currentFrame = self.arView.session.currentFrame, let depthData = currentFrame.sceneDepth {
                self.overlayDepthMap(depthData.depthMap)
            }
            if let currentFrame = self.arView.session.currentFrame {
                // Vision ìš”ì²­ ì‹¤í–‰
                let pixelBuffer = currentFrame.capturedImage
                //self.performModelInference(pixelBuffer: pixelBuffer)
                self.detect(image: CIImage(cvPixelBuffer: pixelBuffer))
                let imageRequestHandler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
                do {
                    try imageRequestHandler.perform(self.requests)
                } catch {
                    print("Failed to perform Vision request: \(error)")
                }
            }
        }
    }

    // Depth map ì˜¤ë²„ë ˆì´ë¥¼ ì¶”ê°€í•˜ëŠ” ë©”ì„œë“œ
    private func overlayDepthMap(_ depthMap: CVPixelBuffer) {
        guard let depthImage = convertDepthDataToUIImage(depthMap) else { return }
        dump(depthMap)

        if depthOverlayView == nil {
            depthOverlayView = UIImageView(frame: arView.bounds)
            depthOverlayView?.contentMode = .scaleAspectFill // ë³€ê²½: ì´ë¯¸ì§€ê°€ ë·°ì˜ ê²½ê³„ë¥¼ ì±„ìš°ë„ë¡ ì„¤ì •
            depthOverlayView?.clipsToBounds = true // ë·° ê²½ê³„ ë°–ì˜ ì´ë¯¸ì§€ ë¶€ë¶„ì„ ì˜ë¼ëƒ„
            depthOverlayView?.alpha = layerAlpha // ë°˜íˆ¬ëª… ì„¤ì •
            arView.addSubview(depthOverlayView!)
        }

        depthOverlayView?.frame = arView.bounds // ARView í¬ê¸°ì— ë§ê²Œ ì¡°ì •
        depthOverlayView?.image = depthImage
    }

    //
    private func convertDepthDataToUIImage(_ depthMap: CVPixelBuffer) -> UIImage? {
        let ciImage = CIImage(cvPixelBuffer: depthMap)
        let context = CIContext(options: nil)
        guard let cgImage = context.createCGImage(ciImage, from: ciImage.extent) else {
            return nil
        }
        return UIImage(cgImage: cgImage, scale: 1.0, orientation: .right)
        //return UIImage(cgImage: cgImage, scale: 1.0, orientation: .up)
    }

    private func updateGridDotsPosition() {
        let screenWidth = arView.bounds.width
        let screenHeight = arView.bounds.height

        for (i, dot) in gridDots.enumerated() {
            let rowIndex = i / col // ê°€ë¡œ ì¤„ ê°œìˆ˜ë¡œ ë‚˜ëˆ”
            let columnIndex = i % col // ê°€ë¡œ ì¤„ ê°œìˆ˜ë¡œ ë‚˜ë¨¸ì§€ ì—°ì‚°

            let x = CGFloat(columnIndex) * screenWidth / CGFloat(col) + screenWidth / CGFloat(2 * col)
            let y = CGFloat(rowIndex) * screenHeight / CGFloat(rw) + screenHeight / CGFloat(2 * rw)
            dot.center = CGPoint(x: x, y: y)
        }
    }

    private func updateDistanceDisplay() {
        let screenWidth = arView.bounds.width
        let screenHeight = arView.bounds.height
        let labelHeight: CGFloat = 20
        let dotSize: CGFloat = 10

        var whiteDotCount = 0
        var redDotCount = 0

        for (i, dot) in gridDots.enumerated() {
            let row = i / col
            let column = i % col
            let x = CGFloat(column) * screenWidth / CGFloat(col) + screenWidth / CGFloat(2 * col)
            let y = CGFloat(row) * screenHeight / CGFloat(rw) + screenHeight / CGFloat(2 * rw)
            let screenPoint = CGPoint(x: x, y: y)

            guard let hitTestResults = arView.hitTest(screenPoint, types: .featurePoint).first else {
                gridLabels[i].text = "N/A"
                dot.backgroundColor = .gray
                continue
            }
            let hitPoint = hitTestResults.worldTransform
            guard let currentFrame = arView.session.currentFrame else { return }
            let cameraPosition = currentFrame.camera.transform
            let distance = calculateDistance(from: cameraPosition, to: hitPoint)

            let dotColor = colorForDistance(distance)
            dot.backgroundColor = dotColor

            if dotColor == .white {
                whiteDotCount += 1
            } else if dotColor == .red {
                redDotCount += 1
            }

            gridLabels[i].text = String(format: "%.2f m", distance)
            gridLabels[i].frame = CGRect(x: x - 50, y: y + dotSize / 2 + labelHeight / 2, width: 100, height: labelHeight)
        }

        let totalDots = col * rw
        if (whiteDotCount+redDotCount) > totalDots / 2 {
            guard let currentFrame = arView.session.currentFrame else { return }
            let pixelBuffer = currentFrame.capturedImage

            if let image = imageP.CVPB2UIImage(pixelBuffer: pixelBuffer) {
                //imageP.UIImage2PhotoLibrary(image)
            }
        }
    }

    // ê±°ë¦¬ì— í•´ë‹¹í•˜ëŠ” ìƒ‰ìƒì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    private func colorForDistance(_ distance: Float) -> UIColor {
        // ê±°ë¦¬ì— ë”°ë¥¸ ìƒ‰ìƒ ë§¤í•‘ ìˆœíšŒ
        for (key, color) in distanceColorMap.sorted(by: { $0.key < $1.key }) {
            if distance <= key {
                return color
            }
        }
        // ë§¤í•‘ëœ ìƒ‰ìƒì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ìƒ‰ìƒ ë°˜í™˜
        return .white
    }

    // ê¸°ë³¸ UIView ê°ì²´ë¥¼ ë°˜í™˜
    func view() -> UIView {
        return arView
    }

    /*
    func detectObjects(in pixelBuffer: CVPixelBuffer) -> [Detection]? {
        do {
            // ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸
            guard let modelPath = Bundle.main.path(forResource: "model_unquant", ofType: "tflite") else { return nil }

            // ê°ì²´ ê°ì§€ ì˜µì…˜ ì„¤ì •
            let options = ObjectDetectorOptions(modelPath: modelPath)

            // ê°ì²´ ê°ì§€ê¸° ìƒì„±
            let detector = try ObjectDetector.detector(options: options)

            // CVPixelBufferë¥¼ MLImageë¡œ ë³€í™˜
            guard let mlImage = MLImage(pixelBuffer: pixelBuffer) else { return nil }

            // ê°ì²´ ê°ì§€ ìˆ˜í–‰
            let detectionResult = try detector.detect(mlImage: mlImage)

            // `DetectionResult`ì—ì„œ í•„ìš”í•œ `[Detection]` ì •ë³´ ì¶”ì¶œ
            return detectionResult.detections
        } catch {
            print("Object detection failed with error: \(error)")
            return nil
        }
    }
    */

    private func loadModel() {
        do {
            let config = MLModelConfiguration()
            let modelURL = Bundle.main.url(forResource: "RamenClassifier", withExtension: "mlmodel")!
            let compiledModelURL = try MLModel.compileModel(at: modelURL)
            let mlModel = try MLModel(contentsOf: compiledModelURL, configuration: config)
            model = try VNCoreMLModel(for: mlModel)
        } catch {
            print("ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: \(error)")
        }
    }
    private func performModelInference(pixelBuffer: CVPixelBuffer) {
        guard let model = self.model else { return }
        let request = VNCoreMLRequest(model: model) { (request, error) in
            if let results = request.results as? [VNClassificationObservation] {
                for result in results {
                    print("Object: \(result.identifier), Confidence: \(result.confidence)")
                }
            }
        }
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
        try? handler.perform([request])
    }

    // CoreMLì˜ CIImageë¥¼ ì²˜ë¦¬í•˜ê³  í•´ì„í•˜ê¸° ìœ„í•œ ë©”ì„œë“œ ìƒì„±, ì´ê²ƒì€ ëª¨ë¸ì˜ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ ì‚¬ìš© ë©ë‹ˆë‹¤.
    func detect(image: CIImage) {
        // CoreMLì˜ ëª¨ë¸ì¸ FlowerClassifierë¥¼ ê°ì²´ë¥¼ ìƒì„± í›„,
        // Vision í”„ë ˆì„ì›Œí¬ì¸ VNCoreMLModel ì»¨í„°ì´ë„ˆë¥¼ ì‚¬ìš©í•˜ì—¬ CoreMLì˜ modelì— ì ‘ê·¼í•œë‹¤.
        guard let coreMLModel = try? RamenClassifier(configuration: MLModelConfiguration()),
              let visionModel = try? VNCoreMLModel(for: coreMLModel.model) else {
            fatalError("Loading CoreML Model Failed")
        }
        // Visionì„ ì´ìš©í•´ ì´ë¯¸ì¹˜ ì²˜ë¦¬ë¥¼ ìš”ì²­
        let request = VNCoreMLRequest(model: visionModel) { request, error in
            guard error == nil else {
                fatalError("Failed Request")
            }
            // ì‹ë³„ìì˜ ì´ë¦„ì„ í™•ì¸í•˜ê¸° ìœ„í•´ VNClassificationObservationë¡œ ë³€í™˜í•´ì¤€ë‹¤.
            guard let classification = request.results as? [VNClassificationObservation] else {
                fatalError("Faild convert VNClassificationObservation")
            }
            // ë¨¸ì‹ ëŸ¬ë‹ì„ í†µí•œ ê²°ê³¼ê°’ í”„ë¦°íŠ¸
            //print(classification)

            // ğŸ‘‰ íƒ€ì´í‹€ì„ ê°€ì¥ ì •í™•ë„ ë†’ì€ ì´ë¦„ìœ¼ë¡œ ì„¤ì •
            if let fitstItem = classification.first {
                print(fitstItem.identifier.capitalized)
            }
        }

        // ì´ë¯¸ì§€ë¥¼ ë°›ì•„ì™€ì„œ performì„ ìš”ì²­í•˜ì—¬ ë¶„ì„í•œë‹¤. (Vision í”„ë ˆì„ì›Œí¬)
        let handler = VNImageRequestHandler(ciImage: image)
        do {
            try handler.perform([request])
        } catch {
            print(error)
        }
    }
}